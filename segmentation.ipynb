{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BPE (Byte Pair Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install subword-nmt\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # download tokenizer\n",
    "\n",
    "with open(\"data/europarl-v9.fi-en.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "   file = f.read() # read file\n",
    "\n",
    "lines = file.splitlines() # split by line\n",
    "tokized_lines = [nltk.word_tokenize(line) for line in lines] # tokenize each line\n",
    "\n",
    "tokenized_text = [\" \".join(tokens) for tokens in tokized_lines] # join tokens with space\n",
    "\n",
    "with open(\"data/tokenized-europarl-v9.fi-en\", \"w\", encoding=\"utf-8\") as f:\n",
    "   f.write(\"\\n\".join(tokenized_text)) # write to file\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! subword-nmt learn-bpe -s 10000 < data/europarl-v9.fi-en.tsv > data/europarl-v9.fi-en.codes\n",
    "! subword-nmt apply-bpe -c data/europarl-v9.fi-en.codes < data/europarl-v9.fi-en.tsv > data/europarl-v9.fi-en.subword"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Only first 10000 lines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 10000 data/tokenized-europarl-v9.fi-en > data/tokenized-europarl-v9.fi-en-10000\n",
    "! subword-nmt learn-bpe -s 10000 < data/tokenized-europarl-v9.fi-en-10000 > data/tokenized-europarl-v9.fi-en-10000.codes\n",
    "! subword-nmt apply-bpe -c data/tokenized-europarl-v9.fi-en-10000.codes < data/tokenized-europarl-v9.fi-en-10000 > data/tokenized-europarl-v9.fi-en-10000.subword\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Morfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install morfessor\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # download tokenizer\n",
    "\n",
    "with open(\"data/europarl-v9.fi\", \"r\", encoding=\"utf-8\") as f:\n",
    "\tfile = f.read() # read file\n",
    "\n",
    "lines = file.splitlines() # split by line\n",
    "tokized_lines = [nltk.word_tokenize(line) for line in lines] # tokenize each line\n",
    "\n",
    "tokenized_text = [\" \".join(tokens) for tokens in tokized_lines] # join tokens with space\n",
    "\n",
    "with open(\"data/tokenized-europarl-v9.fi\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tf.write(\"\\n\".join(tokenized_text)) # write to file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! morfessor -t data/tokenized-europarl-v9.fi -s data/model.bin\n",
    "! morfessor -l data/model.bin -T - < data/tokenized-europarl-v9.fi > data/tokenized-europarl-v9.fi.segmented\n",
    "! morfessor -l data/model.bin -T - --output-newlines --output-format \"{analysis}  \" --output-format-separator \"@@ \" < data/tokenized-europarl-v9.fi > data/tokenized-europarl-v9.fi.segmented"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Only first 10000 lines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 10000 data/tokenized-europarl-v9.fi > data/tokenized-europarl-v9.fi-10000\n",
    "! morfessor -t data/tokenized-europarl-v9.fi-10000 -s data/model.bin\n",
    "! morfessor -l data/model.bin -T - < data/tokenized-europarl-v9.fi-10000 > data/tokenized-europarl-v9.fi-10000.segmented\n",
    "! morfessor -l data/model.bin -T - --output-newlines --output-format \"{analysis}  \" --output-format-separator \"@@ \" < data/tokenized-europarl-v9.fi-10000 > data/tokenized-europarl-v9.fi-10000.segmented"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morfessor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81ade080ac80d5fc5b538897cbc416717c84a98df69c0cfefb787fc6a25b5a4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
