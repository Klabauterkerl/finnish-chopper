{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "https://colab.research.google.com/github/Klabauterkerl/finnish-chopper/blob/main/fairseq_bpe.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k32IU6kfnbn"
      },
      "outputs": [],
      "source": [
        "# Install fairseq and other dependencies\n",
        "%pip install sacrebleu sentencepiece\n",
        "%pip install tensorboardX\n",
        "%pip install subword-nmt\n",
        "# Needed because otherwise fairseq generate will fail bacause of wrong version of pytorch on colab\n",
        "%pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "%pip install fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hehwT42_4HzH",
        "outputId": "5214d06b-3752-4db4-ad05-08cf2590c35c"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths for the mounted Google Drive\n",
        "base_path = \"/content/drive/MyDrive/translation_model\"\n",
        "dataset_path = f\"{base_path}/dataset\"\n",
        "data_bin_path = f\"{base_path}/data-bin\"\n",
        "checkpoints_path = f\"{base_path}/checkpoints\"\n",
        "logs_path = f\"{base_path}/logs\"\n",
        "\n",
        "# Create directories in Google Drive\n",
        "!mkdir -p \"{dataset_path}\"\n",
        "!mkdir -p \"{data_bin_path}\"\n",
        "!mkdir -p \"{checkpoints_path}\"\n",
        "!mkdir -p \"{logs_path}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths when locally running\n",
        "base_path = \"data\"\n",
        "dataset_path = f\"{base_path}/dataset\"\n",
        "data_bin_path = f\"{base_path}/data-bin\"\n",
        "checkpoints_path = f\"{base_path}/checkpoints\"\n",
        "logs_path = f\"{base_path}/logs\"\n",
        "\n",
        "!mkdir -p \"{dataset_path}\"\n",
        "!mkdir -p \"{data_bin_path}\"\n",
        "!mkdir -p \"{checkpoints_path}\"\n",
        "!mkdir -p \"{logs_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEbPx3wLfyDt"
      },
      "outputs": [],
      "source": [
        "# Download and extract dataset\n",
        "!wget -P \"{dataset_path}\" https://www.statmt.org/europarl/v9/training/europarl-v9.fi-en.tsv.gz\n",
        "!gunzip \"{dataset_path}/europarl-v9.fi-en.tsv.gz\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cReXcKyN02ti"
      },
      "outputs": [],
      "source": [
        "# Split dataset into two files, each containing one column of the original dataset\n",
        "\n",
        "!cut -f1 {dataset_path}/europarl-v9.fi-en.tsv > {dataset_path}/train.fi\n",
        "!cut -f2 {dataset_path}/europarl-v9.fi-en.tsv > {dataset_path}/train.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Moses for preprocessing\n",
        "!git clone https://github.com/moses-smt/mosesdecoder.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knhTxHFS14GV"
      },
      "outputs": [],
      "source": [
        "# Train truecaser model for Finnish\n",
        "!mosesdecoder/scripts/recaser/train-truecaser.perl \\\n",
        "-corpus {dataset_path}/train.fi \\\n",
        "-model {dataset_path}/truecase-model.fi\n",
        "\n",
        "# Train truecaser model for English\n",
        "!mosesdecoder/scripts/recaser/train-truecaser.perl \\\n",
        "-corpus {dataset_path}/train.en \\\n",
        "-model {dataset_path}/truecase-model.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_name = \"train\"\n",
        "\n",
        "# Normalize punctuation and tokenize Finnish text\n",
        "!cat {dataset_path}/{set_name}.fi | \\\n",
        "mosesdecoder/scripts/tokenizer/normalize-punctuation.perl fi | \\\n",
        "mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -no-escape -l fi \\\n",
        "> {dataset_path}/{set_name}.tok.fi\n",
        "\n",
        "# Normalize punctuation and tokenize English text\n",
        "!cat {dataset_path}/{set_name}.en | \\\n",
        "mosesdecoder/scripts/tokenizer/normalize-punctuation.perl en | \\\n",
        "mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -no-escape -l en \\\n",
        "> {dataset_path}/{set_name}.tok.en\n",
        "\n",
        "# Truecase the tokenized Finnish text\n",
        "!mosesdecoder/scripts/recaser/truecase.perl \\\n",
        "-model {dataset_path}/truecase-model.fi \\\n",
        "< {dataset_path}/{set_name}.tok.fi \\\n",
        "> {dataset_path}/{set_name}.tok.truecase.fi\n",
        "\n",
        "# Truecase the tokenized English text\n",
        "!mosesdecoder/scripts/recaser/truecase.perl \\\n",
        "-model {dataset_path}/truecase-model.en \\\n",
        "< {dataset_path}/{set_name}.tok.en \\\n",
        "> {dataset_path}/{set_name}.tok.truecase.en\n",
        "\n",
        "# Clean the corpus\n",
        "!perl mosesdecoder/scripts/training/clean-corpus-n.perl \\\n",
        "{dataset_path}/{set_name}.tok.truecase en fi \\\n",
        "{dataset_path}/{set_name}.tok.clean 1 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihQMl1vmu9OX"
      },
      "outputs": [],
      "source": [
        "set_name = \"train\"\n",
        "\n",
        "# Learn a joint BPE model and vocabulary\n",
        "!subword-nmt learn-joint-bpe-and-vocab \\\n",
        "     --input {dataset_path}/{set_name}.tok.clean.fi {dataset_path}/{set_name}.tok.clean.en -s 32000 \\\n",
        "     -o {dataset_path}/bpe.codes --write-vocabulary {dataset_path}/vocab.fi {dataset_path}/vocab.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_name = \"train\"\n",
        "\n",
        "# Apply the learned BPE model and vocabulary to the dev & test set\n",
        "!subword-nmt apply-bpe -c {dataset_path}/bpe.codes \\\n",
        "    --vocabulary {dataset_path}/vocab.fi < {dataset_path}/{set_name}.tok.clean.fi > {dataset_path}/{set_name}.bpe.fi\n",
        "!subword-nmt apply-bpe -c {dataset_path}/bpe.codes \\\n",
        "    --vocabulary {dataset_path}/vocab.en < {dataset_path}/{set_name}.tok.clean.en > {dataset_path}/{set_name}.bpe.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJUK2lKjVvay"
      },
      "outputs": [],
      "source": [
        "# Create Dataset using BPE Data\n",
        "!fairseq-preprocess --source-lang fi --target-lang en \\\n",
        "    --trainpref {dataset_path}/train.bpe --validpref {dataset_path}/dev.bpe --testpref {dataset_path}/test.bpe \\\n",
        "    --destdir {data_bin_path} --workers 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1yS_2poZdHp"
      },
      "outputs": [],
      "source": [
        "# Train Model using BPE Dataset\n",
        "!fairseq-train 'data/data-bin' \\\n",
        "    --arch transformer --share-decoder-input-output-embed \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-tokens 8192 \\\n",
        "    --save-interval 1 \\\n",
        "    --keep-last-epochs 5 --log-format simple --log-interval 100 \\\n",
        "    --tensorboard-logdir 'data/logs' \\\n",
        "    --save-dir 'data/checkpoints' \\\n",
        "    --amp --task translation\\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --patience 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8UjyFVVnDXA"
      },
      "outputs": [],
      "source": [
        "# Generate translations using BPE trained model\n",
        "! fairseq-generate \"{data_bin_path}\" \\\n",
        "    --path {checkpoints_path}/checkpoint_best.pt \\\n",
        "    --batch-size 128 --beam 5 --remove-bpe \\\n",
        "    > {dataset_path}/translations.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjh2Zer4heK4"
      },
      "outputs": [],
      "source": [
        "# Compute BLEU score\n",
        "! fairseq-score --sys \"{dataset_path}/translations.txt\" --ref {dataset_path}/test.en\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
