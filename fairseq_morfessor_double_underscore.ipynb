{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/Klabauterkerl/finnish-chopper/blob/main/fairseq_morfessor.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fairseq and other dependencies\n",
    "%pip install sacrebleu sentencepiece\n",
    "%pip install tensorboardX\n",
    "%pip install subword-nmt\n",
    "%pip install sacremoses\n",
    "# Needed because otherwise fairseq generate will fail bacause of wrong version of pytorch on colab\n",
    "%pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "%pip install fairseq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up filepaths\n",
    "- Exectute the first cell to mount your Google Drive\n",
    "- Execute the second cell to set up the filepaths when runnning locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define paths for the mounted Google Drive\n",
    "base_path = \"/content/drive/MyDrive/translation_model\"\n",
    "dataset_path = f\"{base_path}/dataset\"\n",
    "!mkdir -p \"{dataset_path}\"\n",
    "data_bin_path = f\"{base_path}/data-bin\"\n",
    "checkpoints_path = f\"{base_path}/checkpoints\"\n",
    "logs_path = f\"{base_path}/logs\"\n",
    "morfessor_path = f\"{base_path}/dataset/morfessor\"\n",
    "\n",
    "!mkdir -p \"{dataset_path}\"\n",
    "!mkdir -p \"{data_bin_path}\"\n",
    "!mkdir -p \"{checkpoints_path}\"\n",
    "!mkdir -p \"{logs_path}\"\n",
    "!mkdir -p \"{morfessor_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths when locally running\n",
    "base_path = \"data\"\n",
    "dataset_path = f\"{base_path}/dataset\"\n",
    "data_bin_path = f\"{base_path}/data-bin\"\n",
    "checkpoints_path = f\"{base_path}/checkpoints\"\n",
    "logs_path = f\"{base_path}/logs\"\n",
    "morfessor_path = f\"{base_path}/dataset/morfessor_double_underscore\"\n",
    "\n",
    "!mkdir -p \"{dataset_path}\"\n",
    "!mkdir -p \"{data_bin_path}\"\n",
    "!mkdir -p \"{checkpoints_path}\"\n",
    "!mkdir -p \"{logs_path}\"\n",
    "!mkdir -p \"{morfessor_path}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "- Data used is the Europarl corpus, which can be downloaded from [here](https://www.statmt.org/europarl/v7/fi-en.tgz)\n",
    "- The parallel data is then split into finnish and english files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract dataset\n",
    "!wget -P \"{dataset_path}\" https://www.statmt.org/europarl/v9/training/europarl-v9.fi-en.tsv.gz\n",
    "!gunzip \"{dataset_path}/europarl-v9.fi-en.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into two files, each containing one column of the original dataset\n",
    "!cut -f1 {dataset_path}/europarl-v9.fi-en.tsv > {dataset_path}/europarl-v9.fi\n",
    "!cut -f2 {dataset_path}/europarl-v9.fi-en.tsv > {dataset_path}/europarl-v9.en"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Normalization\n",
    "\n",
    "Before training a Morfessor Model on the provided Dataset the text has to be preprocessed for optimal function.\n",
    "\n",
    "Preprocessing steps taken:\n",
    "- Tokenization\n",
    "- Normalization\n",
    "- Truecasing\n",
    "- Corpus Cleaning (no sentences longer than 50 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train truecaser model for Finnish\n",
    "!mosesdecoder/scripts/recaser/train-truecaser.perl \\\n",
    "-corpus {dataset_path}/train.fi \\\n",
    "-model {dataset_path}/truecase-model.fi\n",
    "\n",
    "# Train truecaser model for English\n",
    "!mosesdecoder/scripts/recaser/train-truecaser.perl \\\n",
    "-corpus {dataset_path}/train.en \\\n",
    "-model {dataset_path}/truecase-model.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"train\"\n",
    "\n",
    "# Normalize punctuation and tokenize Finnish text\n",
    "!cat {dataset_path}/{set_name}.fi | \\\n",
    "mosesdecoder/scripts/tokenizer/normalize-punctuation.perl fi | \\\n",
    "mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -no-escape -l fi \\\n",
    "> {dataset_path}/{set_name}.tok.fi\n",
    "\n",
    "# Normalize punctuation and tokenize English text\n",
    "!cat {dataset_path}/{set_name}.en | \\\n",
    "mosesdecoder/scripts/tokenizer/normalize-punctuation.perl en | \\\n",
    "mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -no-escape -l en \\\n",
    "> {dataset_path}/{set_name}.tok.en\n",
    "\n",
    "# Truecase the tokenized Finnish text\n",
    "!mosesdecoder/scripts/recaser/truecase.perl \\\n",
    "-model {dataset_path}/truecase-model.fi \\\n",
    "< {dataset_path}/{set_name}.tok.fi \\\n",
    "> {dataset_path}/{set_name}.tok.truecase.fi\n",
    "\n",
    "# Truecase the tokenized English text\n",
    "!mosesdecoder/scripts/recaser/truecase.perl \\\n",
    "-model {dataset_path}/truecase-model.en \\\n",
    "< {dataset_path}/{set_name}.tok.en \\\n",
    "> {dataset_path}/{set_name}.tok.truecase.en\n",
    "\n",
    "# Clean the corpus\n",
    "!perl mosesdecoder/scripts/training/clean-corpus-n.perl \\\n",
    "{dataset_path}/{set_name}.tok.truecase en fi \\\n",
    "{dataset_path}/{set_name}.tok.clean 1 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morfesssor preprocessing\n",
    "\n",
    "With Morfessor the Finnish words will be preprocessed by splitting off their suffixes and adding delemiters between the splits\n",
    "\n",
    "This Information can be conatained in Finnish suffixes:\n",
    "- Case\n",
    "- Number\n",
    "- Person\n",
    "- Tense & Mood\n",
    "- Possession\n",
    "- Derivation\n",
    "- Comparative and superlative forms\n",
    "\n",
    "1. Experiment: The delimiter chosen here is \" |\" as it used seldomly in the Finnish language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Morfessor for Morfessor-Encoding\n",
    "#%pip install morfessor\n",
    "\n",
    "# Create Morfessor directory and set its path\n",
    "morfessor_path = f\"{dataset_path}/morfessor\"\n",
    "!mkdir -p \"{morfessor_path}\"\n",
    "\n",
    "# Learn Morfessor model from tokenized data\n",
    "!morfessor -t \"{dataset_path}/train.tok.clean.fi\" -s \"{morfessor_path}/model_fi.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"train\"\n",
    "\n",
    "# Segment Finnish tokenized data using learned Morfessor model\n",
    "!morfessor -l \"data/dataset/morfessor/model_fi.bin\" -T - \\\n",
    "    --output-newlines --output-format \"{analysis}  \" --output-format-separator \" __\" \\\n",
    "    < \"data/dataset/test.tok.clean.fi\" > \\\n",
    "    \"data/dataset/morfessor/test.morfessor_double_underscore.fi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the English data to the Morfessor directory\n",
    "!cp \"{dataset_path}/{set_name}.tok.clean.en\" \"{morfessor_path}/{set_name}.tok.clean.en\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing using BPE\n",
    "\n",
    "After having linguistically preprocessed the text using Morfessor an additonal preprocessing step to make the model more robust is to use Byte Pair Encoding (BPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"train\"\n",
    "\n",
    "# Learn a joint BPE model and vocabulary\n",
    "!subword-nmt learn-joint-bpe-and-vocab \\\n",
    "    --input {morfessor_path}/{set_name}.morfessor_double_underscore.fi {morfessor_path}/{set_name}.tok.clean.en -s 32000 \\\n",
    "    -o {morfessor_path}/bpe_double_underscore.codes --write-vocabulary {morfessor_path}/vocab_double_underscore.fi {morfessor_path}/vocab_double_underscore.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"test\"\n",
    "\n",
    "# Apply the learned BPE model and vocabulary\n",
    "!subword-nmt apply-bpe -c {morfessor_path}/bpe_double_underscore.codes --glossaries \"__\\S+\" \\\n",
    "    --vocabulary {morfessor_path}/vocab_double_underscore.fi < {morfessor_path}/{set_name}.morfessor_double_underscore.fi > {morfessor_path}/{set_name}.bpe_double_underscore.fi\n",
    "!subword-nmt apply-bpe -c {morfessor_path}/bpe_double_underscore.codes --glossaries \"__\\S+\" \\\n",
    "    --vocabulary {morfessor_path}/vocab_double_underscore.en < {morfessor_path}/{set_name}.tok.clean.en > {morfessor_path}/{set_name}.bpe_double_underscore.en"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset using Fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset using BPE Data\n",
    "!fairseq-preprocess --source-lang fi --target-lang en \\\n",
    "    --trainpref {morfessor_path}/train.bpe_double_underscore --validpref {morfessor_path}/dev.bpe_double_underscore --testpref {morfessor_path}/test.bpe_double_underscore \\\n",
    "    --destdir {data_bin_path}/morfessor_double_underscore --workers 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model using BPE Dataset\n",
    "!fairseq-train data/data-bin/morfessor_double_underscore \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --save-interval 1 \\\n",
    "    --keep-last-epochs 5 --log-format simple --log-interval 100 \\\n",
    "    --tensorboard-logdir {logs_path} \\\n",
    "    --save-dir {checkpoints_path} \\\n",
    "    --task translation\\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok moses \\\n",
    "    --eval-bleu-remove-bpe \\\n",
    "    --eval-bleu-print-samples \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "    --patience 3 --amp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translations using BPE trained model\n",
    "! fairseq-generate \"{data_bin_path}\" \\\n",
    "    --path checkpoints/checkpoint_best.pt \\\n",
    "    --batch-size 128 --beam 5 --remove-bpe \\\n",
    "        > {dataset_path}/bpe/translations.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU score\n",
    "!grep ^H {base_path}/translations.txt | cut -f3- > {base_path}/hyp.txt\n",
    "!grep ^T {base_path}/translations.txt | cut -f2- > {base_path}/ref.txt\n",
    "!mosesdecoder/scripts/generic/multi-bleu.perl {base_path}/ref.txt < {base_path}/hyp.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finnish-chopper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
