{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/Klabauterkerl/finnish-chopper/blob/main/fairseq_morfessor.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fairseq and other dependencies\n",
    "%pip install sacrebleu sentencepiece\n",
    "%pip install tensorboardX\n",
    "%pip install subword-nmt\n",
    "%pip install sacremoses\n",
    "# Needed because otherwise fairseq generate will fail bacause of wrong version of pytorch on colab\n",
    "%pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "%pip install fairseq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up filepaths\n",
    "- Exectute the first cell to mount your Google Drive\n",
    "- Execute the second cell to set up the filepaths when runnning locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define paths for the mounted Google Drive\n",
    "base_path = \"/content/drive/MyDrive/translation_model\"\n",
    "dataset_path = f\"{base_path}/dataset\"\n",
    "!mkdir -p \"{dataset_path}\"\n",
    "data_bin_path = f\"{base_path}/data-bin\"\n",
    "checkpoints_path = f\"{base_path}/checkpoints\"\n",
    "logs_path = f\"{base_path}/logs\"\n",
    "morfessor_path = f\"{base_path}/dataset/morfessor\"\n",
    "\n",
    "!mkdir -p \"{dataset_path}\"\n",
    "!mkdir -p \"{data_bin_path}\"\n",
    "!mkdir -p \"{checkpoints_path}\"\n",
    "!mkdir -p \"{logs_path}\"\n",
    "!mkdir -p \"{morfessor_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths when locally running\n",
    "base_path = \"data/morfessor\"\n",
    "dataset_path = f\"{base_path}/dataset\"\n",
    "data_bin_path = f\"{base_path}/data-bin\"\n",
    "checkpoints_path = f\"{base_path}/checkpoints\"\n",
    "logs_path = f\"{base_path}/logs\"\n",
    "morfessor_path = f\"{base_path}/dataset\"\n",
    "evaluation_folder = f\"{base_path}/evaluation\"\n",
    "!mkdir -p \"{dataset_path}\"\n",
    "!mkdir -p \"{data_bin_path}\"\n",
    "!mkdir -p \"{checkpoints_path}\"\n",
    "!mkdir -p \"{logs_path}\"\n",
    "!mkdir -p \"{morfessor_path}\"\n",
    "!mkdir -p \"{evaluation_folder}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "- Data used is the Europarl corpus, which can be downloaded from [here](https://www.statmt.org/europarl/v7/fi-en.tgz)\n",
    "- The parallel data is then split into finnish and english files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract dataset\n",
    "!wget -P \"{dataset_path}\" https://www.statmt.org/europarl/v9/training/europarl-v9.fi-en.tsv.gz\n",
    "!gunzip \"{dataset_path}/europarl-v9.fi-en.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into two files, each containing one column of the original dataset\n",
    "!cut -f1 {dataset_path}/europarl-v9.fi-en.tsv > {dataset_path}/europarl-v9.fi\n",
    "!cut -f2 {dataset_path}/europarl-v9.fi-en.tsv > {dataset_path}/europarl-v9.en"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Normalization\n",
    "\n",
    "Before training a Morfessor Model on the provided Dataset the text has to be preprocessed for optimal function.\n",
    "\n",
    "Preprocessing steps taken:\n",
    "- Tokenization\n",
    "- Normalization\n",
    "- Truecasing\n",
    "- Corpus Cleaning (no sentences longer than 50 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train truecaser model for Finnish\n",
    "!mosesdecoder/scripts/recaser/train-truecaser.perl \\\n",
    "-corpus {dataset_path}/train.fi \\\n",
    "-model {dataset_path}/truecase-model.fi\n",
    "\n",
    "# Train truecaser model for English\n",
    "!mosesdecoder/scripts/recaser/train-truecaser.perl \\\n",
    "-corpus {dataset_path}/train.en \\\n",
    "-model {dataset_path}/truecase-model.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"train\"\n",
    "\n",
    "# Normalize punctuation and tokenize Finnish text\n",
    "!cat {dataset_path}/{set_name}.fi | \\\n",
    "mosesdecoder/scripts/tokenizer/normalize-punctuation.perl fi | \\\n",
    "mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -no-escape -l fi \\\n",
    "> {dataset_path}/{set_name}.tok.fi\n",
    "\n",
    "# Normalize punctuation and tokenize English text\n",
    "!cat {dataset_path}/{set_name}.en | \\\n",
    "mosesdecoder/scripts/tokenizer/normalize-punctuation.perl en | \\\n",
    "mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -no-escape -l en \\\n",
    "> {dataset_path}/{set_name}.tok.en\n",
    "\n",
    "# Truecase the tokenized Finnish text\n",
    "!mosesdecoder/scripts/recaser/truecase.perl \\\n",
    "-model {dataset_path}/truecase-model.fi \\\n",
    "< {dataset_path}/{set_name}.tok.fi \\\n",
    "> {dataset_path}/{set_name}.tok.truecase.fi\n",
    "\n",
    "# Truecase the tokenized English text\n",
    "!mosesdecoder/scripts/recaser/truecase.perl \\\n",
    "-model {dataset_path}/truecase-model.en \\\n",
    "< {dataset_path}/{set_name}.tok.en \\\n",
    "> {dataset_path}/{set_name}.tok.truecase.en\n",
    "\n",
    "# Clean the corpus\n",
    "!perl mosesdecoder/scripts/training/clean-corpus-n.perl \\\n",
    "{dataset_path}/{set_name}.tok.truecase en fi \\\n",
    "{dataset_path}/{set_name}.tok.clean 1 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morfesssor preprocessing\n",
    "\n",
    "With Morfessor the Finnish words will be preprocessed by splitting off their suffixes and adding delemiters between the splits\n",
    "\n",
    "This Information can be conatained in Finnish suffixes:\n",
    "- Case\n",
    "- Number\n",
    "- Person\n",
    "- Tense & Mood\n",
    "- Possession\n",
    "- Derivation\n",
    "- Comparative and superlative forms\n",
    "\n",
    "The delimiter chosen here is \"|\" as it used seldomly in the Finnish language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn Morfessor model from tokenized data\n",
    "!morfessor -t \"{dataset_path}/train.tok.clean.fi\" -s \"{dataset_path}/model_fi.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Finnish tokenized data using learned Morfessor model for train set\n",
    "!morfessor -l \"data/morfessor/dataset/model_fi.bin\" -T - \\\n",
    "    --output-newlines --output-format \"{analysis}  \" --output-format-separator \" __\" \\\n",
    "    < \"data/morfessor/dataset/train.tok.clean.fi\" > \\\n",
    "    \"data/morfessor/dataset/train.morfessor.fi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Finnish tokenized data using learned Morfessor model for dev set\n",
    "!morfessor -l \"data/morfessor/dataset/model_fi.bin\" -T - \\\n",
    "    --output-newlines --output-format \"{analysis}  \" --output-format-separator \" __\" \\\n",
    "    < \"data/morfessor/dataset/dev.tok.clean.fi\" > \\\n",
    "    \"data/morfessor/dataset/dev.morfessor.fi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Finnish tokenized data using learned Morfessor model for test set\n",
    "!morfessor -l \"data/morfessor/dataset/model_fi.bin\" -T - \\\n",
    "    --output-newlines --output-format \"{analysis}  \" --output-format-separator \" __\" \\\n",
    "    < \"data/morfessor/dataset/test.tok.clean.fi\" > \\\n",
    "    \"data/morfessor/dataset/test.morfessor.fi\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing using BPE\n",
    "\n",
    "After having linguistically preprocessed the text using Morfessor an additonal preprocessing step to make the model more robust is to use Byte Pair Encoding (BPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"train\"\n",
    "\n",
    "# Learn a joint BPE model and vocabulary\n",
    "!subword-nmt learn-joint-bpe-and-vocab \\\n",
    "    --input {dataset_path}/{set_name}.morfessor.fi {dataset_path}/{set_name}.tok.clean.en -s 32000 \\\n",
    "    -o {dataset_path}/bpe.codes --write-vocabulary {dataset_path}/vocab.fi {dataset_path}/vocab.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"train\"\n",
    "\n",
    "# Apply the learned BPE model and vocabulary\n",
    "!subword-nmt apply-bpe -c {dataset_path}/bpe.codes \\\n",
    "    --vocabulary {dataset_path}/vocab.fi < {dataset_path}/{set_name}.morfessor.fi > {dataset_path}/{set_name}.bpe.fi\n",
    "!subword-nmt apply-bpe -c {dataset_path}/bpe.codes \\\n",
    "    --vocabulary {dataset_path}/vocab.en < {dataset_path}/{set_name}.tok.clean.en > {dataset_path}/{set_name}.bpe.en"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset using Fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset using BPE Data\n",
    "!fairseq-preprocess --source-lang fi --target-lang en \\\n",
    "    --trainpref {dataset_path}/train.bpe --validpref {dataset_path}/dev.bpe --testpref {dataset_path}/test.bpe \\\n",
    "    --destdir {data_bin_path} --workers 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix CUDA_VISIBLE_DEVICES= <insert index of GPUs to use> if some GPUs are occupied\n",
    "\n",
    "# Train Model using BPE Dataset\n",
    "!fairseq-train data/morfessor/data-bin \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --save-interval 1 \\\n",
    "    --keep-last-epochs 5 --log-format simple --log-interval 100 \\\n",
    "    --tensorboard-logdir data/morfessor/logs \\\n",
    "    --save-dir data/morfessor/checkpoints \\\n",
    "    --task translation\\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok moses \\\n",
    "    --eval-bleu-remove-bpe \\\n",
    "    --eval-bleu-print-samples \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "    --max-epoch 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translations using trained model\n",
    "!fairseq-generate {data_bin_path} \\\n",
    "    --path {checkpoints_path}/checkpoint_best.pt \\\n",
    "    --batch-size 128 --beam 5 --remove-bpe \\\n",
    "    --scoring sacrebleu --sacrebleu\\\n",
    "        > {base_path}/translations_sacrebleu.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f'{base_path}/translations_sacrebleu.txt'  # File generated by fairseq-generate\n",
    "reordered_output_file = f'{base_path}/reordered_output.txt'  # File to save the reordered translations\n",
    "\n",
    "# Read the output file and extract translations\n",
    "with open(output_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "translations = {}\n",
    "for line in lines:\n",
    "    if line.startswith('H-'):\n",
    "        parts = line.split('\\t')\n",
    "        index = int(parts[0].split('-')[1])\n",
    "        translation = parts[2].strip()\n",
    "        translations[index] = translation\n",
    "\n",
    "# Reorder translations and save to file\n",
    "with open(reordered_output_file, 'w') as f:\n",
    "    for i in sorted(translations.keys()):\n",
    "        f.write(translations[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the truecasing of the reference test set (English)\n",
    "!mosesdecoder/scripts/recaser/detruecase.perl \\\n",
    "< {dataset_path}/test.tok.clean.en \\\n",
    "> {evaluation_folder}/test.tok.en\n",
    "\n",
    "# Reverse the truecasing of the source test set (Finnish)\n",
    "!mosesdecoder/scripts/recaser/detruecase.perl \\\n",
    "< {dataset_path}/test.tok.clean.fi \\\n",
    "> {evaluation_folder}/test.tok.fi\n",
    "\n",
    "# Reverse the tokenization of the reference test set (English)\n",
    "!mosesdecoder/scripts/tokenizer/detokenizer.perl -l en \\\n",
    "< {evaluation_folder}/test.tok.en \\\n",
    "> {evaluation_folder}/test.detok.en\n",
    "\n",
    "# Reverse the tokenization of the source test set (Finnish)\n",
    "!mosesdecoder/scripts/tokenizer/detokenizer.perl -l fi \\\n",
    "< {evaluation_folder}/test.tok.fi \\\n",
    "> {evaluation_folder}/test.detok.fi\n",
    "\n",
    "# Reverse the punctuation normalization of the reference test set (English)\n",
    "!mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -r \\\n",
    "< {evaluation_folder}/test.detok.en \\\n",
    "> {evaluation_folder}/test.en\n",
    "\n",
    "# Reverse the punctuation normalization of the source test set (Finnish)\n",
    "!mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -r \\\n",
    "< {evaluation_folder}/test.detok.fi \\\n",
    "> {evaluation_folder}/test.fi\n",
    "\n",
    "# Reverse the truecasing of the hypothesis translations (English)\n",
    "!mosesdecoder/scripts/recaser/detruecase.perl \\\n",
    "< {base_path}/reordered_output.txt \\\n",
    "> {evaluation_folder}/reordered_output.truecase.txt\n",
    "\n",
    "# Reverse the tokenization of the hypothesis translations (English)\n",
    "!mosesdecoder/scripts/tokenizer/detokenizer.perl -l en \\\n",
    "< {evaluation_folder}/reordered_output.truecase.txt \\\n",
    "> {evaluation_folder}/reordered_output.detok.txt\n",
    "\n",
    "# Reverse the punctuation normalization of the hypothesis translations (English)\n",
    "!mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -r \\\n",
    "< {evaluation_folder}/reordered_output.detok.txt \\\n",
    "> {evaluation_folder}/reordered_output.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sacrebleu {evaluation_folder}/test.en < {evaluation_folder}/reordered_output.txt > {evaluation_folder}/sacrebleu_score.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!comet-score -t {evaluation_folder}/reordered_output.txt -r {evaluation_folder}/test.en -s {evaluation_folder}/test.fi > {evaluation_folder}/comet_score.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finnish-chopper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
