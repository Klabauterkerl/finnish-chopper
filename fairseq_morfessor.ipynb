{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/Klabauterkerl/finnish-chopper/blob/main/fairseq_morfessor.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fairseq and other dependencies\n",
    "%pip install sacrebleu sentencepiece\n",
    "%pip install tensorboardX\n",
    "%pip install subword-nmt\n",
    "%pip install sacremoses\n",
    "# Needed because otherwise fairseq generate will fail bacause of wrong version of pytorch on colab\n",
    "%pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "%pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define paths for the mounted Google Drive\n",
    "base_path = \"/content/drive/MyDrive/translation_model\"\n",
    "dataset_path = f\"{base_path}/dataset\"\n",
    "!mkdir -p \"{dataset_path}\"\n",
    "data_bin_path = f\"{base_path}/data-bin\"\n",
    "checkpoints_path = f\"{base_path}/checkpoints\"\n",
    "logs_path = f\"{base_path}/logs\"\n",
    "\n",
    "!mkdir -p \"{dataset_path}\"\n",
    "!mkdir -p \"{data_bin_path}\"\n",
    "!mkdir -p \"{checkpoints_path}\"\n",
    "!mkdir -p \"{logs_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths when locally running\n",
    "base_path = \"data\"\n",
    "dataset_path = f\"{base_path}/dataset\"\n",
    "data_bin_path = f\"{base_path}/data-bin\"\n",
    "checkpoints_path = f\"{base_path}/checkpoints\"\n",
    "logs_path = f\"{base_path}/logs\"\n",
    "\n",
    "!mkdir -p \"{dataset_path}\"\n",
    "!mkdir -p \"{data_bin_path}\"\n",
    "!mkdir -p \"{checkpoints_path}\"\n",
    "!mkdir -p \"{logs_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract dataset\n",
    "!wget -P \"{dataset_path}\" https://www.statmt.org/europarl/v9/training/europarl-v9.fi-en.tsv.gz\n",
    "!gunzip \"{dataset_path}/europarl-v9.fi-en.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into two files, each containing one column of the original dataset\n",
    "!cut -f1 {dataset_path}/europarl-v9.fi-en.tsv > {dataset_path}/europarl-v9.fi\n",
    "!cut -f2 {dataset_path}/europarl-v9.fi-en.tsv > {dataset_path}/europarl-v9.en"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Normalization\n",
    "\n",
    "Before training a Morfessor Model on the provided Dataset the text has to be preprocessed for optimal function.\n",
    "\n",
    "Preprocessing steps taken:\n",
    "- Tokenization\n",
    "- Normalization\n",
    "- Truecasing\n",
    "- Corpus Cleaning (no sentences longer than 50 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train truecaser model for Finnish\n",
    "!mosesdecoder/scripts/recaser/train-truecaser.perl \\\n",
    "-corpus {dataset_path}/train.fi \\\n",
    "-model {dataset_path}/truecase-model.fi\n",
    "\n",
    "# Train truecaser model for English\n",
    "!mosesdecoder/scripts/recaser/train-truecaser.perl \\\n",
    "-corpus {dataset_path}/train.en \\\n",
    "-model {dataset_path}/truecase-model.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"train\"\n",
    "\n",
    "# Normalize punctuation and tokenize Finnish text\n",
    "!cat {dataset_path}/{set_name}.fi | \\\n",
    "mosesdecoder/scripts/tokenizer/normalize-punctuation.perl fi | \\\n",
    "mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -no-escape -l fi \\\n",
    "> {dataset_path}/{set_name}.tok.fi\n",
    "\n",
    "# Normalize punctuation and tokenize English text\n",
    "!cat {dataset_path}/{set_name}.en | \\\n",
    "mosesdecoder/scripts/tokenizer/normalize-punctuation.perl en | \\\n",
    "mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -no-escape -l en \\\n",
    "> {dataset_path}/{set_name}.tok.en\n",
    "\n",
    "# Truecase the tokenized Finnish text\n",
    "!mosesdecoder/scripts/recaser/truecase.perl \\\n",
    "-model {dataset_path}/truecase-model.fi \\\n",
    "< {dataset_path}/{set_name}.tok.fi \\\n",
    "> {dataset_path}/{set_name}.tok.truecase.fi\n",
    "\n",
    "# Truecase the tokenized English text\n",
    "!mosesdecoder/scripts/recaser/truecase.perl \\\n",
    "-model {dataset_path}/truecase-model.en \\\n",
    "< {dataset_path}/{set_name}.tok.en \\\n",
    "> {dataset_path}/{set_name}.tok.truecase.en\n",
    "\n",
    "# Clean the corpus\n",
    "!perl mosesdecoder/scripts/training/clean-corpus-n.perl \\\n",
    "{dataset_path}/{set_name}.tok.truecase en fi \\\n",
    "{dataset_path}/{set_name}.tok.clean 1 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morfesssor preprocessing\n",
    "\n",
    "With Morfessor the Finnish words will be preprocessed by splitting off their suffixes and adding delemiters between the splits\n",
    "\n",
    "This Information can be conatained in Finnish suffixes:\n",
    "- Case\n",
    "- Number\n",
    "- Person\n",
    "- Tense & Mood\n",
    "- Possession\n",
    "- Derivation\n",
    "- Comparative and superlative forms\n",
    "\n",
    "The delimiter chosen here is \"|\" as it used seldomly in the Finnish language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Morfessor for Morfessor-Encoding\n",
    "%pip install morfessor\n",
    "\n",
    "# Create Morfessor directory and set its path\n",
    "morfessor_path = f\"{dataset_path}/morfessor\"\n",
    "!mkdir -p \"{morfessor_path}\"\n",
    "\n",
    "# Learn Morfessor model from tokenized data\n",
    "!morfessor -t \"{dataset_path}/train.tok.clean.fi\" -s \"{morfessor_path}/model_fi.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"train\"\n",
    "\n",
    "# Segment Finnish tokenized data using learned Morfessor model\n",
    "!morfessor -l \"/content/drive/MyDrive/translation_model/dataset/morfessor/model_fi.bin\" -T - \\\n",
    "    --output-newlines --output-format \"{analysis}  \" --output-format-separator \" |\" \\\n",
    "    < \"/content/drive/MyDrive/translation_model/dataset/{set_name}.tok.clean.fi\" > \\\n",
    "    \"/content/drive/MyDrive/translation_model/dataset/morfessor/{set_name}.morfessor.fi\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing using BPE\n",
    "\n",
    "After having linguistically preprocessed the text using Morfessor an additonal preprocessing step to make the model more robust is to use Byte Pair Encoding (BPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"train\"\n",
    "\n",
    "# Learn a joint BPE model and vocabulary\n",
    "!subword-nmt learn-joint-bpe-and-vocab \\\n",
    "    --input {morfessor_path}/{set_name}.morfessor.fi {dataset_path}/{set_name}.tok.clean.en -s 32000 \\\n",
    "    -o {morfessor_path}/bpe.codes --write-vocabulary {morfessor_path}/vocab.fi {morfessor_path}/vocab.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"dev\"\n",
    "\n",
    "# Apply the learned BPE model and vocabulary\n",
    "!subword-nmt apply-bpe -c {morfessor_path}/bpe.codes \\\n",
    "    --vocabulary {morfessor_path}/vocab.fi < {morfessor_path}/{set_name}.morfessor.fi > {morfessor_path}/{set_name}.bpe.fi\n",
    "!subword-nmt apply-bpe -c {morfessor_path}/bpe.codes \\\n",
    "    --vocabulary {morfessor_path}/vocab.en < {morfessor_path}/{set_name}.tok.clean.en > {morfessor_path}/{set_name}.bpe.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset using BPE Data\n",
    "!fairseq-preprocess --source-lang fi --target-lang en \\\n",
    "    --trainpref {dataset_path}/train.bpe --validpref {dataset_path}/dev.bpe --testpref {dataset_path}/test.bpe \\\n",
    "    --destdir {data_bin_path} --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model using BPE Dataset\n",
    "!fairseq-train {data_bin_path} \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8192 \\\n",
    "    --save-interval 1 \\\n",
    "    --keep-last-epochs 5 --log-format simple --log-interval 100 \\ \n",
    "    --tensorboard-logdir {logs_path} \\\n",
    "    --save-dir {checkpoints_path} \\\n",
    "    --amp \\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok moses \\\n",
    "    --eval-bleu-remove-bpe \\\n",
    "    --eval-bleu-print-samples \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate translations using BPE trained model\n",
    "! fairseq-generate \"{data_bin_path}\" \\\n",
    "    --path checkpoints/checkpoint_best.pt \\\n",
    "    --batch-size 128 --beam 5 --remove-bpe \\\n",
    "        > {dataset_path}/bpe/translations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BLEU score\n",
    "!grep ^H {base_path}/translations.txt | cut -f3- > {base_path}/hyp.txt\n",
    "!grep ^T {base_path}/translations.txt | cut -f2- > {base_path}/ref.txt\n",
    "!mosesdecoder/scripts/generic/multi-bleu.perl {base_path}/ref.txt < {base_path}/hyp.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finnish-chopper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
